---
title: "Assignment 1"
author: "Oguzhan Yetkin, Oleg Litvinov, Victor Retamal, group 4"
date: "15 March 2022"
output:
  pdf_document: default
  html_document:
    df_print: paged
highlight: tango
fontsize: 11pt
editor_options: 
  chunk_output_type: inline
  markdown: 
    wrap: 72
---

## Exercise 1. Post-operative nausea.
The file contains data about post-operative nausea after medication against nausea. Two different medicines were administered to patients that complained about post-operative nausea. One of the medicines, Pentobarbital, was administered in two different doses.

```{r}
nausea_df <- read.table("data/nauseatable.txt", header=TRUE)
nausea_df
```
### a)  Discuss whether a contingency table test is appropriate here. If yes, perform this test in order to test whether the different medicines work equally well against nausea. Where are the main inconsistencies?

There are two factors: presence of nausea and the medication. For each combination of factors, the number of cases are registered. Contingency table test is applicable in terms of the task to find the dependency between the factors. For that a specific condition has to be met.

```{r}
z=chisq.test(nausea_df)
z
```
There are no contraindications for the chi-square test. The test concludes that there is a dependence between row and column variables. Let's check what is that difference.

```{r}
library(corrplot)
z$residuals
corrplot(z$residuals, is.cor = FALSE)
```
Chlorpromazine is relatevily more helpful in terms of fighting against nausea in comparison to both dosages of Pentobarbital. Also, 100mg of Pentobarbital has more nausea cases. 

### b)  Perform a permutation test in order to test whether the different medicines work equally well against nausea. Permute the medicine labels for this purpose. Use as test statistic the chisquare test statistic for contingency tables, which can be extracted from the output of the command chisq.test. (Hint: make a data frame in R consisting of two columns. One column should contain an indicator whether or not the patient in that row suffered from nausea, and the other column should indicate the medicine.)

```{r}
indicator_col <- c()
label_col <- c()
for(i in 1:3){
  indicator_col <- append(indicator_col, rep(0, nausea_df[i, 1]))
  indicator_col <- append(indicator_col, rep(1, nausea_df[i, 2]))
  label_col <- append(label_col, rep(rownames(nausea_df)[i], rowSums(nausea_df[i, ])))
}

nausea_two_col_df <- data.frame(indicator_col, label_col)
head(nausea_two_col_df)
```
```{r}

mystat <- function(x) chisq.test(x)$statistic
B <- 1000
tstar <- numeric(B)
for(i in 1:B){
  perm_label <- sample(nausea_two_col_df$label_col) ## permuting the labels
  tstar[i] <- mystat(table(data.frame(nausea_two_col_df$indicator_col, perm_label)))
}
myt <- mystat(table(data.frame(nausea_two_col_df$indicator_col, nausea_two_col_df$label_col)))

pl <- sum(tstar<myt)/B
pr <- sum(tstar>myt)/B
p_perm <- min(pl, pr)
p_perm
```
The permutation test rejects the null hypethesis that different medicines work equally well against nausea.

### c)  Compare the p-value found by the permutation test with the p-value found from the chisquare test for contingency tables. Explain the difference/equality of the two p-values.

Relaunch of the permutation test retrieves the p-value of about 0.03-0.04 while the p-value from the chi-square test is 0.036. The permutation test is completely suitable for such kind of tasks. It also reveals the same conclusion and similar p-value as the chi-square test.

## Exercise 2. Airpollution.

The data were obtained to determine predictors related to air pollution. We want to investigate which explanatory variables need to be included into a linear regression model with oxidant as the response variable.

```{r}
pollution_df <- read.table("data/airpollution.txt", header=TRUE)
head(pollution_df)
```


### a)  Make some graphical summaries of the data. Investigate the problem of potential and influence points, and the problem of collinearity.

```{r}
# cor(pollution_df, method = c("spearman"))
if (!require("corrplot")) install.packages("corrplot")
library(corrplot)
par(mfrow=c(2, 1))
res <- cor(pollution_df, method='kendall')
corrplot(res, type = "upper", order = "hclust", 
         tl.col = "black", tl.srt = 45)
pairs(pollution_df, pch = 19)
```
The command order(abs(residuals(model))) gives the indices of the ordered absolute values of residuals from smallest to largest.
```{r}
pollution_lm = lm(oxidant~insolation+humidity+wind+temperature+day, data=pollution_df)
order(abs(residuals(pollution_lm)))
```
The mean shift outlier model can be applied to test whether the k-th point significantly deviates from the other points in a linear regression setting.
```{r}
u_out=rep(0, nrow(pollution_df)); u_out[21]=1
pollution_lm_outlier=lm(oxidant~insolation+humidity+wind + u_out, data=pollution_df); summary(pollution_lm_outlier)
```
Only the 21 outlier is significant.

The Cook’s distance Di quantifies the influence of observation i on the predictions:
$D_{i}=\frac{1}{(p+1) \hat{\sigma}^{2}} \sum_{j=1}^{n}\left(\hat{Y}_{(i), j}-\hat{Y}_{j}\right)^{2}$

```{r}
plot(1:nrow(pollution_df), cooks.distance(pollution_lm), type="b")
```
Rule of thumb: if the Cook’s distance for some data point is close to or larger than 1, it is considered to be an influence point. So the point 23 is an influence here.

```{r}
if (!require("psych")) install.packages("psych")
library(psych)
pairs.panels(pollution_df, 
             method = "spearman", # correlation method
             hist.col = "#00AFBB",
             density = TRUE,  # show density plots
             ellipses = TRUE # show correlation ellipses
             )
```


### b)  Use the added variable plot to depict the relationship between response oxidant and predictor wind. What is the meaning of the slope of fitted regression for this scatter plot?

```{r}
if (!require("car")) install.packages("car")
library(car)

attach(pollution_df)
mod = lm(oxidant~insolation+humidity+wind)
par(mfrow=c(2, 1))
avPlots(mod)
summary(mod)
```
The slopes on the plots reflect the regression coefficients from the original multiple regression model mod. 
```{r}
y = residuals(lm(pollution_df$oxidant~pollution_df$temperature+pollution_df$insolation+pollution_df$humidity))
x = residuals(lm(pollution_df$wind~pollution_df$temperature+pollution_df$insolation+pollution_df$humidity))
plot(x, y, xlab='residual of wind', ylab='residual of oxidant')
```

### c)  Fit a linear regression model to the data. Use both the step-up and step-down methods to find the best model. If step-up and step-down yield two different models, choose one and motivate your choice. 

#### Step-up

```{r}

for(i in names(pollution_df)){
  if(i == 'oxidant'){next}
  # summary(lm(oxidant~i))
  print(summary(lm(paste('pollution_df$oxidant',  '~pollution_df$', i))))
}
```
Wind variable gives maximum increase in the R^2. The variable is significant. Therefore, we can continue.

```{r}
for(i in names(pollution_df)){
  if(i %in% c('oxidant', 'wind')){next}
  print(summary(lm(paste('pollution_df$oxidant',  '~pollution_df$wind+pollution_df$', i))))
}
```
Temperature variable works in the same way as the previous choice. Continue.

```{r}
for(i in names(pollution_df)){
  if(i %in% c('oxidant', 'wind', 'temperature')){next}
  print(summary(lm(paste('pollution_df$oxidant',  
                         '~pollution_df$wind',
                         '+pollution_df$temperature',
                         '+pollution_df$', 
                         i))))
}
```
Humidity has the highest R-squared increase but the variable is not significant. Therefore, we don't add it to the model. Resulting model is:
```{r}
print(summary(lm(pollution_df$oxidant~pollution_df$wind+pollution_df$temperature)))
```

oxidant = -5.2 - 0.4*wind + 0.5*temperature + error, with R-squared = 0.8.

#### Step-down

```{r}
summary(lm(pollution_df$oxidant~ pollution_df$wind + pollution_df$temperature + pollution_df$day + pollution_df$humidity + pollution_df$insolation))
```
Day has the largest p-value and the value is larger than 0.05. Removing it.

```{r}
summary(lm(pollution_df$oxidant ~ pollution_df$wind + pollution_df$temperature + pollution_df$humidity + pollution_df$insolation))
```
Insolation is the largers from the insignificant. Removing it.

```{r}
summary(lm(pollution_df$oxidant~ pollution_df$wind + pollution_df$temperature + pollution_df$humidity))
```
Humidity is the only insignificant. Removing.

```{r}
summary(lm(pollution_df$oxidant~ pollution_df$wind + pollution_df$temperature))
```
All remaining variables are significant. Resulting model: oxidant = -5.2 - 0.4*wind + 0.5*temperature + error, with R-squared = 0.8. The model is the same as obtained with the step-up approach.

d)  Determine 95% confidence and prediction intervals for oxidant using the model you preferred in c) for wind=33, temperature=54, humidity=77 and insolation=21.

```{r}
x1 <- pollution_df$wind
x2 <- pollution_df$temperature

mod = lm(pollution_df$oxidant ~ x1 + x2)

newxdata = data.frame(x1=33, x2=54)

predict(mod, newxdata, interval='prediction', level=0.95)
predict(mod, newxdata, interval='confidence', level=0.95)
```

## Exercise 3. Fruit flies.
To investigate the effect of sexual activity on longevity of fruit flies, 75 male fruit flies were divided randomly in three groups of 25. The fruit flies in the first group were kept solitary, those in the second were kept together with one virgin female fruit fly per day, and those in the third group were kept together with eight virgin female fruit flies a day. In the data-file three groups are labelled isolated, low and high. The number of days until death (longevity) was measured for all flies. Later, it was decided to measure also the length of their thorax. Add a column loglongevity to the data-frame, containing the logarithm of the number of days until death. *Use this as the response variable in the following*.

```{r}
flies_df <- read.table("data/fruitflies.txt", header=TRUE)
flies_df$loglongevity=log(flies_df$longevity)
head(flies_df)
```
```{r}
par(mfrow=c(1, 2))
hist(flies_df$longevity)
hist(flies_df$loglongevity)
```
```{r}
shapiro.test(flies_df$longevity)
shapiro.test(flies_df$loglongevity)
```
The original column is normal while the log of it is not.
```{r}
boxplot(longevity~activity, data=flies_df)
```
```{r}
plot(loglongevity~thorax, pch=as.character(activity), data=flies_df)
```


### a)  Make an informative plot of the data. Investigate whether sexual activity influences longevity by performing a statistical test, without taking the thorax length into account. What are the estimated longevities for the three conditions? Comment.

```{r}
if (!require("dplyr")) install.packages("dplyr")
library(dplyr)

par(mfrow=c(2, 1))
res <- cor(select_if(flies_df, is.numeric), method='spearman')
corrplot(res, type = "upper", order = "hclust", 
         tl.col = "black", tl.srt = 45)
pairs(select_if(flies_df, is.numeric), pch = 19)
```
```{r}
flies_df$activity=as.factor(flies_df$activity)

flies_lm = lm(loglongevity ~ activity, data=flies_df)
anova(flies_lm)
```
H0 is rejected. Sexual activity influences longevity.

```{r}
summary(flies_lm)
```
estimated longevities for the three conditions: 
```{r}
high = 3.60212
isolated = high + 0.51722
low = high + 0.39711
high; isolated; low
```

### b)  Investigate whether sexual activity influences longevity by performing a statistical test, now including thorax length as an explanatory variable into the analysis. Does sexual activity increase or decrease longevity? What are the estimated longevities for the three groups, for flies with the minimal and maximal thorax lengths?

```{r}
ancova_lm = lm(loglongevity ~ activity + thorax, data=flies_df) ## order matters but we use drop so don't care. latter have to be of interest
drop1(ancova_lm, test="F")
```
```{r}
summary(ancova_lm)
```
From the coefficients we can conclude that more sex - shorter life.
```{r}
shapiro.test(residuals(ancova_lm))
```


```{r}
par(mfrow=c(1, 2))
qqnorm(residuals(ancova_lm)); qqline(residuals(ancova_lm))
plot(fitted(ancova_lm), residuals(ancova_lm))
```


```{r}
ancova_lm_int = lm(loglongevity ~ activity * thorax, data=flies_df)
anova(ancova_lm_int)
```
There is no significant interaction between factor activity and predictor thorax.

```{r}
summary(ancova_lm_int)
```
The interaction term is not significant. So, no indication that the initial analysis (ancova_lm without interaction) is in trouble.

```{r}
# predict(ancova_lm, flies_df[which.min(flies_df$thorax), c('thorax', 'activity')])
# predict(ancova_lm, flies_df[which.max(flies_df$thorax), c('thorax', 'activity')])

min_thorax = min(flies_df$thorax)
max_thorax = max(flies_df$thorax)

# min_high = predict(ancova_lm, data.frame(thorax = min_thorax, activity = "high"))
min_high = 1.21893 + 2.97899 * min_thorax
max_high = 1.21893 + 2.97899 * max_thorax

min_iso = 1.21893 + 0.40998 + 2.97899 * min_thorax
max_iso = 1.21893 + 0.40998 + 2.97899 * max_thorax

min_low = 1.21893 + 0.28570 + 2.97899 * min_thorax
max_low = 1.21893 + 0.28570 + 2.97899 * max_thorax

exp(min_high); exp(max_high)
exp(min_iso); exp(max_iso)
exp(min_low); exp(max_low)
```

### c)  How does thorax length influence longevity? Investigate graphically and by using an appropriate test whether this dependence is similar under all three conditions of sexual activity.

```{r}
plot(loglongevity~thorax, pch=as.character(activity), data=flies_df)
for (i in unique(flies_df$activity)) abline(lm(loglongevity~thorax, data=flies_df[flies_df$activity==i,]))
```

```{r}
ancova_lm_int = lm(loglongevity ~ activity * thorax, data=flies_df)
anova(ancova_lm_int)
```
Only the last p-value is relevant which always concerns interaction for models with interaction. We conclude from it that $H_0: \beta_1 = \beta_2$ is not rejected, i.e., there is no interaction between factor activity and predictor thorax.

### d)  Which of the two analyses, without or with thorax length, do you prefer? Is one of the analyses wrong?

Both analyses case to the conclusion that sexual activity influences longevity, more sex - shorter life.

### e)  Perform the ancova analysis with the number of days as the response, rather than its logarithm. Was it wise to use the logarithm as response?

```{r}
ancova_lm = lm(longevity ~ activity + thorax, data=flies_df) ## order matters but we use drop so don't care. latter have to be of interest
drop1(ancova_lm, test="F")
```

```{r}
par(mfrow=c(1, 2))
qqnorm(residuals(ancova_lm)); qqline(residuals(ancova_lm))
plot(fitted(ancova_lm), residuals(ancova_lm))
```
```{r}
shapiro.test(residuals(ancova_lm))
```
Residuals are normal.

It's wise. For both columns the residuals are normal. If they were not, ANCOVA would not be relevant. For non-log model the residuals look non-homogenious.

##Exercise 4. Personalized system of instruction
The data was collected to study the effect of a new teaching method called “personalized system of instruction” (psi), 32 students were randomized to either receive psi or to be taught using the existing method. At the end of the teaching period the success of the teaching method was assessed by giving the students a difficult assignment, which they could pass or not. The average grade of the students were also available for analysis: gpa on a scale of 0–4, with 4 being the best grade.

```{r}
psi_df <- read.table("data/psi.txt", header=TRUE)
head(psi_df)
```
```{r, fig.width=5,fig.height=3}
par(mfrow=c(1, 2))
boxplot(gpa ~ passed, data=psi_df)
boxplot(gpa ~ psi, data=psi_df)
```
```{r}
xtabs(~psi+passed, data=psi_df)
```
```{r}
is.numeric(psi_df$passed)
is.numeric(psi_df$gpa)
```


### a)  Fit a logistic regression model with both explanatory variables, perform relevant tests. Does psi work?

```{r}
psi_glm = glm(passed ~ psi + gpa, data=psi_df, family=binomial)
summary(psi_glm)
```
The R-function glm (generalized linear model) is used instead of lm to create the glm
object. The option family=binomial overrules the default normal model (which gives
lm). The 2 explanatory variables are inserted here as numerical.

```{r}
drop1(psi_glm, test="Chisq")
```

```{r}
psi_df$psi = as.factor(psi_df$psi)
psi_glm2 = glm(passed ~ psi + gpa, data=psi_df, family=binomial)
drop1(psi_glm2, test="Chisq")
```
```{r}
summary(psi_glm2)$coefficients
```

psi really works

### b)  Estimate the probability that a student with a gpa equal to 3 who receives psi passes the assignment. Estimate the same probability for a student who does not receive psi. Comment.


```{r}
# Response gives you the numerical result while class gives you the label assigned to that value.
# exp(-11 + 2.33*1 + 3.06*3)
predict(psi_glm2, data.frame(psi="1", gpa=3), type="response")
predict(psi_glm2, data.frame(psi="0", gpa=3), type="response")
```


### c)  Estimate the relative change in odds of passing the assignment rendered by instructing students with psi rather than the standard method (for an arbitrary student). What is the interpretation of this number? Is it dependent on gpa?

With psi: 2.337776. 

```{r}
psi_glm3 = glm(passed ~ psi * gpa, data=psi_df, family=binomial)
drop1(psi_glm3, test="Chisq")
```
There is no interaction between psi and gpa.

### d)  Propose and perform an alternative method of analysis based on contingency tables. Compare its results to the results of the first approach.

```{r}
matrix = table(psi_df[, c('psi', 'passed')])

# assumption is not met -> bootstrap
z=chisq.test(matrix, simulate.p.value=TRUE)
z
```

There are no contraindications for the chi-square test. The test concludes that there is a dependence between row and column variables. Fisher is also applicable as the table is 2x2.

```{r}
fisher.test(matrix)
```
Same conclusion. psi and passed are dependent.

e)  Given the way the experiment was conducted, is this second approach wrong? Name both an advantage and a disadvantage of the two approaches, relative to each other.

Assumption for chi-square is not met. In first we can include numeric variable